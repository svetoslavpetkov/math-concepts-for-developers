{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939bc27b-beb3-40b8-8fc5-fb41e5733a59",
   "metadata": {},
   "source": [
    "# HyperLogLog algorithm for cardinality estimation\n",
    "\n",
    "## Summary\n",
    "\n",
    "## Terms\n",
    "\n",
    "* The term \"**cardinality**\" is used to mean the number of distinct elements in a data stream with repeated elements. Cardinality estimation is the task of determining the number of distinct elements in a data stream y estimation\n",
    "* The term \n",
    "\n",
    "## Problem: Cardinality estimation on huge datasets\n",
    "\n",
    "### Calculating distinct count\n",
    "    Calculating a distinc count in a given dataset has a trivial implemention. There are structures in modern languages that makes implmentation easy.\n",
    "\n",
    "```typescript\n",
    "// psudo code\n",
    "const map = new Map();\n",
    "\n",
    "for(record in ordersDataset) {\n",
    "    map.add(record.customerName)\n",
    "}\n",
    "\n",
    "consoloe.log(\"The distinc count of customers in the dataset is\" + map.size)\n",
    "\n",
    "```\n",
    "The Map implementation may varry form $ \\mathcal{O}(log(n)) $ to $ \\mathcal{O}(n) $.\n",
    "    This means the complexyty along with the dataset elements itearation will be $ \\mathcal{O}(n^2)  $. **Not so bad !**. Not at all, but we are calculating only the computational complexity.\n",
    "\n",
    "### What about memory footprint ?\n",
    "\n",
    "    In order to calculate all distinc values,we need to keep all of them in memory. Despide the fact the Map/Hashtable structures are blazing fast, they have to keep either the hashed value or both hashed and real value in memory. This depends on the implmentation of the hastable and also of the value hashing function implementation. \n",
    "   Let us have a try. Let us assume that:\n",
    "   * We are using MD5 has function which results in $128 bits = 16 Bytes$\n",
    "* our values will be strings with average length of 50 chatarcters,  charset UTF-8. This means a single character is using $32 Bits = 4 Bytes$\n",
    "* our hastabe stores the has and the real values, so that it handles hash collisions well\n",
    "\n",
    "Based on the given data we can calculate the following formula for estimating memory footprint:\n",
    "\n",
    "$$ size(text) = len(text)*4 Bytes $$\n",
    "\n",
    "$$ AllocatedMemeory = \\sum_{i=1}^{n} (size(text_n) + 16)$$\n",
    "\n",
    "This means that calculating the exact cardinality of the distinct elements in a dataset requires an amount of memory proportional to the cardinality, which is impractical for very large data sets\n",
    "\n",
    "Single string memory footpring will be $ 50 * 5 + 16 = 266 Bytes$. Sample memory foorpints based on distinct count memebers:\n",
    "\n",
    "| Dataset cardinality | Memory size (Byte) | Fromated memory size |\r\n",
    "|---------------------|--------------------|----------------------|\r\n",
    "| 100000              | 26600000           | 26,60 MB             |\r\n",
    "| 1000000             | 266000000          | 266,00 MB            |\r\n",
    "| 1000000000          | 266000000000       | 266,00 GB             | So we need at lea6t 256 GB Memory the supportive Hashtable(Map) structure alone in order to calculateting the distinct count of a co valueslumn in a single dataset wtih cardina\n",
    "\n",
    "### Can we reuse the already calculated cardianlity and cache it ?\n",
    "Yes we can - we can keep and memoize that for a given dataset and column customers we have $5*10^5$ distinct memebers. This has its advantages,  but it has one significant disatvantage: we cannot combine the cardinality of two separate cached datasets. So if **A** and **B** are two set:\n",
    "$$ A \\neq B  $$\n",
    "$$ cardinality(A) + cardinality(B) \\neq carinality(A \\cup B) $$\n",
    "   \n",
    " \n",
    "\n",
    "## Count-distinct estimaion\n",
    "Cardinality estimation is a prediction of the distinct count values over a dataset column orIt is used where we want to sacriface 100% correctness of the results in order to get performance in terms of memory or cache optimization.\n",
    "\n",
    "Examples:\n",
    "* What is the estimated distinc count of customers over an or\n",
    "* What is the etimated distinc count of words in a given tex\n",
    "* Biological dat analisysa*  dataset.\n",
    "* Cardinality etimation is used by SQL engines Query planners (not only) to predict how many distinct elements exists in given dataset. This information is used to generate optimal queryg\n",
    ".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8895c-62e3-4c25-956c-a34fc3961b92",
   "metadata": {},
   "source": [
    "## Hyper Log Log Algorithm\n",
    "\n",
    "### Hyper Log Log definittion\n",
    "> The basis of the HyperLogLog algorithm is the observation that the cardinality of a multiset of uniformly distributed random numbers can be estimated by calculating the maximum number of leading zeros in the binary representation of each number in the set. If the maximum number of leading zeros observed is n, an estimate for the number of distinct elements in the set is $2^n$\n",
    ">\n",
    "> In the HyperLogLog algorithm, a hash function is applied to each element in the original multiset to obtain a multiset of uniformly distributed random numbers with the same cardinality as the original multiset. The cardinality of this randomly distributed set can then be estimated using the algorithm above.\n",
    ">\n",
    "> The simple estimate of cardinality obtained using the algorithm above has the disadvantage of a large variance. In the HyperLogLog algorithm, the variance is minimised by splitting the multiset into numerous subsets, calculating the maximum number of leading zeros in the numbers in each of these subsets, and using a harmonic mean to combine these estimates for each subset into an estimate of the cardinality of the whole set.\n",
    "> \n",
    "> -- <cite>[wikipedia][HLL_wikipedia]</cite>\n",
    "\n",
    "\n",
    "### Memory footprint\n",
    "\n",
    "The new HLL algorithm makes it possible to estimate cardinalities well beyond $10^9$ with a typical error rate of 2% while using a memory of only 1.5 kilobytes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[HLL_wikipedia]: https://en.wikipedia.org/wiki/HyperLogog\n",
    "\n",
    "### Hyper log log operations\n",
    "The HyperLogLog has three main operations:\n",
    "1. **add** -> adds a new element to the set\n",
    "2. **count** -> obtains the cardinality of the set\n",
    "3. **merge** -> obtains the union of two sets\n",
    "\n",
    "We can define some more (derived operations) which combines the merge of two sets and count on them:\n",
    "* **cardinality of the intersection**\n",
    "* **cardinality of the difference**\n",
    "\n",
    "#### HLL add operation\n",
    "The add operation consists of computing the hash of the input data $v$ with a hash function $h$, getting the first $b$ bits (where $b$ is $log_2(m)$ ), and  and adding 1 to them to obtain the address of the register to modify. With the remaining bits compute $\\rho(\\omega)$ which returns the position of the leftmost 1, where leftmost position is 1 (in other words: number of leading zeros plus 1). The new value of the register will be the maximum between the current value of the register and $\\rho(\\omega)$\n",
    "\n",
    "$$x := h(v)$$\n",
    "$$j:= 1 + \\langle{x_1x_2...x_b}\\rangle_2$$\n",
    "$$\\omega := x_{b+1}x_{b+2}...$$\n",
    "$$M[j]:=max(M[j],\\rho(\\omega))$$\n",
    "\n",
    "#### HLL Count operation\n",
    "\n",
    "The count algorithm operation consists in computing the harmonic mean of the $m$ registers, and using a constant to derive an estimate $E$ of the count:\n",
    "$$Z = \t\\bigg( {\\sum_{j=1}^m{2^{-M[j]}}} \\bigg)^{-1} $$\n",
    "$$\\alpha_m = \\bigg( m\\int_0^\\infty \\bigg( log_2{\\bigg( \\cfrac{2 + u}{1 + u} \\bigg)}\\bigg)^m du  \\bigg)^{-1} $$\n",
    "$$E = \\alpha_mM^2Z$$\n",
    "\n",
    "The intuition is that $n$ being the unknown cardinality of $M$, each subset $M_j$ will have $n/m$ elements. Then for $max_{x \\in M_j}(\\rho(x))$ should be close to $log_2(m/n)$. The harmonic mean of 2 to theese quanitites is $mZ$ which should be near $m/n$. Thus, $m^2Z \\simeq n$. Finally the nostant $\\alpha_m$ is introduced to correct a systematic multicaptive bias present in $m^2Z$ due to hash collisions\n",
    "\n",
    "According to the [HLL the analysis of a near-optimal cardinality estimation algorithm][HLL the analysis of a near-optimal cardinality estimation algorithm]\n",
    "The constant $\\alpha_m$ is not simple to calculate and can be \n",
    "\n",
    "[HLL the analysis of a near-optimal cardinality estimation algorithm]: https://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf \"HyperLogLog: the analysis of a near-optimal \n",
    "cardinality estimation algorithm\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e910595-b76d-4bab-915a-d525459221ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
