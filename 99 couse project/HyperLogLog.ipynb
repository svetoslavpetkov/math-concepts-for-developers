{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939bc27b-beb3-40b8-8fc5-fb41e5733a59",
   "metadata": {},
   "source": [
    "# HyperLogLog algorithm for cardinality estimation\n",
    "\n",
    "## Problem: Cardinality estimation on huge datasets\n",
    "\n",
    "### Calculating distinct count\n",
    "    Calculating a distinc count in a given dataset has a trivial implemention. There are structures in modern languages that makes implmentation easy.\n",
    "\n",
    "```typescript\n",
    "// psudo code\n",
    "const map = new Map();\n",
    "\n",
    "for(record in ordersDataset) {\n",
    "    map.add(record.customerName)\n",
    "}\n",
    "\n",
    "consoloe.log(\"The distinc count of customers in the dataset is\" + map.size)\n",
    "\n",
    "```\n",
    "The Map implementation may varry form $ \\mathcal{O}(log(n)) $ to $ \\mathcal{O}(n) $.\n",
    "    This means the complexyty along with the dataset elements itearation will be $ \\mathcal{O}(n^2)  $. **Not so bad !**. Not at all, but we are calculating only the computational complexity.\n",
    "\n",
    "### What about memory footprint ?\n",
    "\n",
    "    In order to calculate all distinc values,we need to keep all of them in memory. Despide the fact the Map/Hashtable structures are blazing fast, they have to keep either the hashed value or both hashed and real value in memory. This depends on the implmentation of the hastable and also of the value hashing function implementation. \n",
    "   Let us have a try. Let us assume that:\n",
    "   * We are using MD5 has function which results in $128 bits = 16 Bytes$\n",
    "* our values will be strings with average length of 50 chatarcters,  charset UTF-8. This means a single character is using $32 Bits = 4 Bytes$\n",
    "* our hastabe stores the has and the real values, so that it handles hash collisions well\n",
    "\n",
    "Based on the given data we can calculate the following formula for estimating memory footprint:\n",
    "\n",
    "$$ size(text) = len(text)*4 Bytes $$\n",
    "\n",
    "$$ AllocatedMemeory = \\sum_{i=1}^{n} (size(text_n) + 16)$$\n",
    "\n",
    "This means that calculating the exact cardinality of the distinct elements in a dataset requires an amount of memory proportional to the cardinality, which is impractical for very large data sets\n",
    "\n",
    "Single string memory footpring will be $ 50 * 5 + 16 = 266 Bytes$. Sample memory foorpints based on distinct count memebers:\n",
    "\n",
    "| Dataset cardinality | Memory size (Byte) | Fromated memory size |\r\n",
    "|---------------------|--------------------|----------------------|\r\n",
    "| 100000              | 26600000           | 26,60 MB             |\r\n",
    "| 1000000             | 266000000          | 266,00 MB            |\r\n",
    "| 1000000000          | 266000000000       | 266,00 GB            |   | So we need at lea6t 256 GB Memory the supportive Hashtable(Map) structure alone in order to calculateting the distinct count of a co valueslumn in a single dataset wtih cardinality 1 Billl \n",
    "\n",
    "### Can we reuse the already calculated cardianlity and cache it ?\n",
    "Yes we can - we can keep and memoize that for a given dataset and column customers we have $5*10^5$ distinct memebers. This has its advantages,  but it has one significant disatvantage: we cannot combine the cardinality of two separate cached datasets. So if $A$ and $B$ are two set:\n",
    "$$ A \\neq B  $$\n",
    "$$ cardinality(A) + cardinality(B) \\neq carinality(A \\cup B) $$i\n",
    "n. |\n",
    "   \n",
    " \n",
    "\n",
    "\n",
    "## What is Cardinalit\n",
    "Cardinality estimation is the task of determining the number \n",
    "of distinct elements in a data stream.y estimation\n",
    "Cardinality estimation is a prediction of the distinct count values over a dataset column orIt is used where we want to sacriface 100% correctness of the results in order to get performance.\n",
    "\n",
    " attribute. Examples:\n",
    "> What is the estimated distinc count of customers over an ordersWhat is the etimated distinc count of words in a given text\n",
    "> Biological data dataset.\n",
    "> \n",
    "Cardinality etimation is used by SQL engines Query planners (not only) to predict how many distinct elements exists in given dataset. This information is used to generate optimal query execution plan.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d30003-217c-43bb-8c05-c6ff7a43f224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
